#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Alvaro del Castillo
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, 51 Franklin Street, Fifth Floor, Boston, MA 02110-1335, USA.
#
# Authors:
#     Alvaro del Castillo <alvaro.delcastillo@gmail.com>
#
# TODO: All this code are mainly notebooks extracted and pasted here. It needs a refactor once it is consolidated.


import argparse
import logging
import sys

import numpy as np
import pandas as pd

from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split


MLSAMPLES_USAGE_MSG = \
    """%(prog)s [-g] [<args>] | --help"""

MLSAMPLES_DESC_MSG = \
    """Machine learning samples."""

MLSAMPLES_EPILOG_MSG = \
    """Run '%(prog)s --help' to show the help for using the program."""

# Logging formats
MLSAMPLES_LOG_FORMAT = "[%(asctime)s] - %(message)s"
MLSAMPLES_DEBUG_LOG_FORMAT = "[%(asctime)s - %(name)s - %(levelname)s] - %(message)s"

# Pre-trained model to be used
MODELS_DIR = "models"
RESNET_DIR = MODELS_DIR + "/resnet50"
RESNET_H5 = RESNET_DIR + "/resnet50_weights_tf_dim_ordering_tf_kernels.h5"
RESNET_JSON = RESNET_DIR + "/imagenet_class_index.json"

DATA_DIR = "data"

IMAGES_DIR = "images"
IMAGE_DIR_TRAIN = IMAGES_DIR + "/train/"
IMAGE_DIR_TEST = IMAGES_DIR + "/test/"


def configure_logging(debug=True):
    """Configure MyMLSAMPLES logging
    The function configures the log messages produced by machine_learning.
    By default, log messages are sent to stderr. Set the parameter
    `debug` to activate the debug mode.
    :param debug: set the debug mode
    """
    if not debug:
        logging.basicConfig(level=logging.INFO,
                            format=MLSAMPLES_LOG_FORMAT)
        logging.getLogger('requests').setLevel(logging.WARNING)
        logging.getLogger('urrlib3').setLevel(logging.WARNING)
    else:
        logging.basicConfig(level=logging.DEBUG,
                            format=MLSAMPLES_DEBUG_LOG_FORMAT)


def parse_args():
    """Parse command line arguments"""

    parser = argparse.ArgumentParser(usage=MLSAMPLES_USAGE_MSG,
                                     description=MLSAMPLES_DESC_MSG,
                                     epilog=MLSAMPLES_EPILOG_MSG,
                                     formatter_class=argparse.RawDescriptionHelpFormatter,
                                     add_help=False)

    parser.add_argument('-h', '--help', action='help',
                        help=argparse.SUPPRESS)

    return parser.parse_args()


# Function for comparing different models
def score_model(model, X_train, X_valid, y_train, y_valid):
    from sklearn.metrics import mean_absolute_error

    model.fit(X_train, y_train)
    preds = model.predict(X_valid)
    return mean_absolute_error(y_valid, preds)


# Function for comparing different approaches
def score_dataset(X_train, X_valid, y_train, y_valid):
    model = RandomForestRegressor(n_estimators=100, random_state=0)
    model.fit(X_train, y_train)
    preds = model.predict(X_valid)
    return mean_absolute_error(y_valid, preds)

def handle_missing_values():
    # The goal is to show the different strategies for handling features with missing values
    # Strategies: drop the feature values (column), fill with a default value (imputation) and imputation+mark it

    # Read the data
    X_full = pd.read_csv(DATA_DIR + '/input/home-data-for-ml-course/train.csv', index_col='Id')
    X_test_full = pd.read_csv(DATA_DIR + '/input/home-data-for-ml-course/test.csv', index_col='Id')

    # Remove rows with missing target, separate target from predictors
    X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)
    y = X_full.SalePrice
    X_full.drop(['SalePrice'], axis=1, inplace=True)

    # To keep things simple, we'll use only numerical predictors
    X = X_full.select_dtypes(exclude=['object'])
    X_test = X_test_full.select_dtypes(exclude=['object'])

    # Break off validation set from training data
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,
                                                          random_state=0)

    # Drop columns with missing values
    cols_with_missing = [col for col in X_train.columns
                         if X_train[col].isnull().any()]

    # Fill in the lines below: drop columns in training and validation data
    reduced_X_train = X_train.drop(cols_with_missing, axis=1)
    reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)

    print("MAE (Drop columns with missing values):")
    print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))

    # Imputation
    my_imputer = SimpleImputer()
    imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))
    imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))

    # Imputation removed column names; put them back
    imputed_X_train.columns = X_train.columns
    imputed_X_valid.columns = X_valid.columns

    print("MAE (Imputation):")
    print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))


def kaggle_sample2_ml():
    # Predict house prices for Ames, Iowa
    # https://www.kaggle.com/learn/intermediate-machine-learning

    # Data from: https://www.kaggle.com/c/home-data-for-ml-course/data
    # Read the data
    X_full = pd.read_csv(DATA_DIR + '/input/home-data-for-ml-course/train.csv', index_col='Id')
    X_test_full = pd.read_csv(DATA_DIR + '/input/home-data-for-ml-course/test.csv', index_col='Id')

    # Obtain target and predictors
    print(X_full.columns)

    y = X_full.SalePrice
    features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']
    features += ['KitchenAbvGr']
    X = X_full[features].copy()
    X_test = X_test_full[features].copy()

    # Break off validation set from training data
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,
                                                          random_state=0)
    print(X_train.head())

    # Evaluate several models
    from sklearn.ensemble import RandomForestRegressor

    # Define the models
    model_1 = RandomForestRegressor(n_estimators=50, random_state=0)
    model_2 = RandomForestRegressor(n_estimators=100, random_state=0)
    model_3 = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)
    model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)
    model_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)

    models = [model_1, model_2, model_3, model_4, model_5]

    print("Evaluating between different RandomForestRegressor")
    for i in range(0, len(models)):
        mae = score_model(models[i], X_train, X_valid, y_train, y_valid)
        print("Model %d MAE: %d" % (i + 1, mae))

    model_31 = RandomForestRegressor(n_estimators=80, criterion='mae', random_state=0)
    model_32 = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)
    model_33 = RandomForestRegressor(n_estimators=110, criterion='mae', random_state=0)
    model_34 = RandomForestRegressor(n_estimators=120, criterion='mae', random_state=0)
    model_35 = RandomForestRegressor(n_estimators=130, criterion='mae', random_state=0)
    model_36 = RandomForestRegressor(n_estimators=200, criterion='mae', random_state=0)
    model_37 = RandomForestRegressor(n_estimators=300, criterion='mae', random_state=0)

    models3 = [model_31, model_32, model_33, model_34, model_35, model_36, model_37]

    print("Evaluating in the same RandomForestRegressor with <>n_estimators")
    for i in range(0, len(models3)):
        mae = score_model(models3[i], X_train, X_valid, y_train, y_valid)
        print("Model %d MAE: %d" % (i + 1, mae))

    my_model = model_3

    # Fit the model to the training data: all train and validation
    my_model.fit(X, y)

    # Generate test predictions
    preds_test = my_model.predict(X_test)

    # Save predictions in format used for competition scoring
    output = pd.DataFrame({'Id': X_test.index,
                           'SalePrice': preds_test})
    output.to_csv('submission.csv', index=False)


def kaggle_sample_ml():
    from sklearn.tree import DecisionTreeRegressor

    # Sample on predicting houses prices with ML
    # https://www.kaggle.com/dansbecker/melbourne-housing-snapshot/downloads/melb_data.csv/5
    melbourne_file_path = DATA_DIR + '/input/melbourne-housing-snapshot/melb_data.csv'
    melbourne_data = pd.read_csv(melbourne_file_path)
    print(melbourne_data.columns)
    # dropna drops missing values (think of na as "not available")
    melbourne_data = melbourne_data.dropna(axis=0)
    y = melbourne_data.Price
    melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']
    X = melbourne_data[melbourne_features]
    print(X.describe())
    print(X.head())

    # Define model. Specify a number for random_state to ensure same results each run
    melbourne_model = DecisionTreeRegressor(random_state=1)
    # Fit model
    melbourne_model.fit(X, y)
    print("Making predictions for the following 5 houses:")
    print(X.head())
    print("The predictions are")
    print(melbourne_model.predict(X.head()))
    from sklearn.metrics import mean_absolute_error
    predicted_home_prices = melbourne_model.predict(X)
    # MAE
    print(mean_absolute_error(y, predicted_home_prices))
    # Let's use some data to train some to validate
    from sklearn.model_selection import train_test_split
    # split data into training and validation data, for both features and target
    # The split is based on a random number generator. Supplying a numeric value to
    # the random_state argument guarantees we get the same split every time we
    # run this script.
    train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)
    melbourne_model = DecisionTreeRegressor()
    melbourne_model.fit(train_X, train_y)
    val_predictions = melbourne_model.predict(val_X)
    print(mean_absolute_error(val_y, val_predictions))

    # Play with different models to find the best one
    def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):
        model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
        model.fit(train_X, train_y)
        preds_val = model.predict(val_X)
        mae = mean_absolute_error(val_y, preds_val)
        return (mae)

    for max_leaf_nodes in [5, 50, 500, 5000]:
        my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)
        print("Max leaf nodes: %d  \t\t Mean Absolute Error:  %d" % (max_leaf_nodes, my_mae))

    from sklearn.ensemble import RandomForestRegressor
    forest_model = RandomForestRegressor(random_state=1)
    forest_model.fit(train_X, train_y)
    melb_preds = forest_model.predict(val_X)
    print(mean_absolute_error(val_y, melb_preds))


def kaggle_digit_dl():
    # Detecting hand written digits
    # A new model is created from scratch
    # The model can be improved changing: number of layers,
    # kernel_size or filter in the convolution layers
    # Download test data from:
    # https://www.kaggle.com/c/digit-recognizer/download/test.csv
    #

    from sklearn.model_selection import train_test_split
    from tensorflow.python import keras
    from tensorflow.python.keras.models import Sequential
    from tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout

    DIR_TRAIN = IMAGES_DIR + "/train/"
    # DIR_TEST = IMAGES_DIR + "/digits/test/"

    img_rows, img_cols = 28, 28
    num_classes = 10

    def data_prep(raw):
        out_y = keras.utils.to_categorical(raw.label, num_classes)

        num_images = raw.shape[0]
        x_as_array = raw.values[:, 1:]
        x_shaped_array = x_as_array.reshape(num_images, img_rows, img_cols, 1)
        out_x = x_shaped_array / 255
        return out_x, out_y

    train_file = DIR_TRAIN + "train.csv"
    raw_data = pd.read_csv(train_file)

    x, y = data_prep(raw_data)

    # Creating a new sequential neural model
    model = Sequential()
    model.add(Conv2D(20, kernel_size=(3, 3),
                     activation='relu',
                     input_shape=(img_rows, img_cols, 1)))
    model.add(Conv2D(20, kernel_size=(3, 3), activation='relu'))
    model.add(Flatten())
    # Helper layer than improve the predictions
    model.add(Dense(128, activation='relu'))
    # Output layer, with num_classes predictions
    model.add(Dense(num_classes, activation='softmax'))

    model.compile(loss=keras.losses.categorical_crossentropy,
                  optimizer='adam',
                  metrics=['accuracy'])
    model.summary()
    model.to_json()
    # x training data, y predictions
    model.fit(x, y,
              batch_size=128,
              epochs=2,
              validation_split=0.2)


def kaggle_sample_dl():
    # For deep learning we must to cover
    # Howto to use pre created models
    # Howto use transfer learning to reuse models in new similar topics
    # Howto create new models
    from os.path import join

    img_paths = [join(IMAGE_DIR_TRAIN, filename) for filename in
                 ['0246f44bb123ce3f91c939861eb97fb7.jpg',
                  '84728e78632c0910a69d33f82e62638c.jpg',
                  '8825e914555803f4c67b26593c9d5aff.jpg',
                  '91a5e8db15bccfb6cfa2df5e8b95ec03.jpg']]

    img_paths_mina = [IMAGES_DIR + "/mina/mina1.jpg"]

    from tensorflow.python.keras.applications.resnet50 import preprocess_input
    from tensorflow.python.keras.preprocessing.image import load_img, img_to_array

    image_size = 224

    def read_and_prep_images(img_paths, img_height=image_size, img_width=image_size):
        imgs = [load_img(img_path, target_size=(img_height, img_width)) for img_path in img_paths]
        img_array = np.array([img_to_array(img) for img in imgs])
        output = preprocess_input(img_array)
        return output

    from tensorflow.python.keras.applications import ResNet50

    my_model = ResNet50(weights=RESNET_H5)
    test_data = read_and_prep_images(img_paths)
    preds = my_model.predict(test_data)

    from learntools.deep_learning.decode_predictions import decode_predictions
    from PIL import Image

    most_likely_labels = decode_predictions(preds, top=3, class_list_path=RESNET_JSON)

    for i, img_path in enumerate(img_paths):
        Image.open(img_path).show()
        print(most_likely_labels[i])


def main():
    args = parse_args()

    configure_logging()

    logging.info("Starting machine_learning samples.")

    # TODO: add params to execute a specific sample
    # kaggle_sample_dl()
    # kaggle_digit_dl()
    # kaggle_sample_ml()
    # kaggle_sample2_ml()
    handle_missing_values()

    logging.info("Samples execution finished..")


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        s = "\n\nReceived Ctrl-C or other break signal. Exiting.\n"
        sys.stderr.write(s)
        sys.exit(0)